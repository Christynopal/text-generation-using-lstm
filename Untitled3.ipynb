{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltp--YNRZ6cz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf # Import tensorflow module\n",
        "\n",
        "\n",
        "\n",
        "# Example text data\n",
        "text = \"i am christy, i am from lalgudi.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create sequences of tokens\n",
        "input_sequences = []\n",
        "for line in text.split('.'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Create predictors and labels\n",
        "X, y = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya5uRHmDaV4O",
        "outputId": "ee76267e-a3b5-413d-8baf-4e386d787958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 6, 100)            600       \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 150)               150600    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 6)                 906       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 152106 (594.16 KB)\n",
            "Trainable params: 152106 (594.16 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X, y, epochs=100, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q9Y2ecSaZ0m",
        "outputId": "e89f1c75-b8f4-4ea9-80d8-842249cc782b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.7915 - accuracy: 0.1667\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.7780 - accuracy: 0.5000\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.7643 - accuracy: 0.5000\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.7501 - accuracy: 0.5000\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.7350 - accuracy: 0.3333\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.7187 - accuracy: 0.3333\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.7006 - accuracy: 0.3333\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.6806 - accuracy: 0.3333\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.6581 - accuracy: 0.3333\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.6328 - accuracy: 0.3333\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.6046 - accuracy: 0.3333\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.5734 - accuracy: 0.3333\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.5397 - accuracy: 0.3333\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.5049 - accuracy: 0.3333\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.4711 - accuracy: 0.3333\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.4410 - accuracy: 0.3333\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.4149 - accuracy: 0.3333\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3884 - accuracy: 0.3333\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3554 - accuracy: 0.5000\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3139 - accuracy: 0.5000\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2666 - accuracy: 0.5000\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2178 - accuracy: 0.5000\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1710 - accuracy: 0.5000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1274 - accuracy: 0.6667\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0861 - accuracy: 0.6667\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0449 - accuracy: 0.8333\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0015 - accuracy: 0.8333\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9548 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9057 - accuracy: 0.8333\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.8576 - accuracy: 0.6667\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.8145 - accuracy: 0.6667\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.7769 - accuracy: 0.6667\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7389 - accuracy: 0.6667\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6966 - accuracy: 0.6667\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6534 - accuracy: 0.6667\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6136 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5750 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5339 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.4946 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4651 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4391 - accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4100 - accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.3843 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.3599 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.3318 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.3059 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2850 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2601 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.2344 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2139 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1933 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.1730 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1571 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1408 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1249 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1128 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1018 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0914 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0826 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0743 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0666 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0602 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0548 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0496 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0449 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0410 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0375 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0341 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0312 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0287 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0264 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0243 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0224 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0209 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0194 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0181 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0169 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0158 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0149 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0140 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0132 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0125 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0118 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0106 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0101 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0097 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0060 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_text(seed_text, next_words, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "seed_text = \"i am christy,\"\n",
        "next_words = 4\n",
        "\n",
        "print(generate_text(seed_text, next_words, max_sequence_len))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C5d5J4TahZ2",
        "outputId": "0a2953a7-4785-411b-a3a3-7816d4a9e1d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am christy, i am from lalgudi\n"
          ]
        }
      ]
    }
  ]
}